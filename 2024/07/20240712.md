<head><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" /><meta charset="UTF-8"></head>

# 20240712

I feel a bit tired, mainly because I researched how to optimize resources in machine-learning model inference very hard. Let\'s relax on the weekend.

---

## [1992A. Only Pluses](https://codeforces.com/contest/1992/problem/A)

Let\'s consider a simpler situation in which I get to incrase one of the three numbers by 1.

`a <= b <= c`

I have three options

- `(a + 1) * b * c` = `abc + bc`
- `a * (b + 1) * c` = `abc + ac`
- `a * b * (c + 1)` = `abc + ab`

Because a <= b <= c, bc >= ac >= ab. Therefore, `(a + 1) * b * c` is the largest option, i.e., increaseing the smallest number of three maximizing their product.

Is it guaranteed that maximizing the product for each step leads to the maximum value in the end after applying the operation 5 times? I don\'t know...

---

`torch.no_grad()` to disable gradient calculation and save memory.

> Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires\_grad=True.

I tried with our project, and (only) see a slight improvement on the memory usage. Or, it\'s totally possible that I misunderstood something and applied it in the wrong way (I don\'t see many web pages on the function usage).

Also, I was browing the web pages of TorchScript, but couldn\'t figure it out.

---

Cheese 10g

Total carbohydrate 10g

---

MUST:

- Finish "Just for Fun"
- Social Interaction @ Sushi Maru

TODO:

- Update my resume with the sign up feature
- Update my resume with the document structure and AWS S3
- Update my LinkedIn summary section

---

[index](../../index.html)
[20240711](20240711.html)
[20240713](20240713.html)
